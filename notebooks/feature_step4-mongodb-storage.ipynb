{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc44c5b-5efd-4495-9826-171215d87524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (13.0.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (2.8.2)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.11/site-packages (from pyarrow) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil) (1.16.0)\n",
      "Downloading pymongo-4.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.7.0 pymongo-4.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo pyarrow python-dateutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3eb3ecc-d243-49c9-957b-8b2bd1bf9098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb://mongouser:mongopassword@mongodb:27017/taxi_logs?authSource=admin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "USER = \"mongouser\"\n",
    "PWD  = \"mongopassword\"\n",
    "HOST = \"mongodb\"\n",
    "DB   = \"taxi_logs\"   # your target DB for writes\n",
    "\n",
    "# Try admin as the auth DB (common when the root user was created via MONGO_INITDB_* envs)\n",
    "os.environ[\"MONGO_URL\"] = f\"mongodb://{USER}:{PWD}@{HOST}:27017/{DB}?authSource=admin\"\n",
    "print(os.environ[\"MONGO_URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5be92e-6b68-4a7a-a547-6180d03c94ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(os.environ[\"MONGO_URL\"])\n",
    "print(client.admin.command(\"ping\"))  # should print {'ok': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadaec0-a5c0-4aab-a3b1-dd4e226f49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet_to_mongo.py\n",
    "import os\n",
    "from datetime import timezone\n",
    "from pymongo import MongoClient, InsertOne\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "from dateutil import parser as dtparse\n",
    "import re\n",
    "\n",
    "os.environ[\"PARQUET_ROOT\"] = \"/home/jovyan/work/data/nyc-taxi/partitioned/year=2019\"\n",
    "\n",
    "# If Mongo has no auth:\n",
    "os.environ[\"MONGO_URL\"] = \"mongodb://mongouser:mongopassword@mongodb:27017/taxi_logs?authSource=admin\"\n",
    "\n",
    "\n",
    "MONGO_URL = os.getenv(\"MONGO_URL\", \"mongodb://localhost:27017/\")\n",
    "PARQUET_ROOT = os.getenv(\"PARQUET_ROOT\", \"/data/taxi\")  # e.g., mounted host path or HDFS copy to local\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"5000\"))\n",
    "\n",
    "client = MongoClient(MONGO_URL)\n",
    "db = client[\"taxi_logs\"]\n",
    "col = db[\"trips\"]\n",
    "\n",
    "# Define a tiny transformer from an Arrow batch to list[dict]\n",
    "def batch_to_docs(batch: pa.RecordBatch, partition_values: dict[str, str] | None = None):\n",
    "    # Columns expected in NYC taxi parquet\n",
    "    cols = {name: batch.column(i) for i, name in enumerate(batch.schema.names)}\n",
    "    # Some datasets have timestamps as strings; some as timestamp types. Normalize to ISO strings then to datetimes.\n",
    "    pick_col = cols.get(\"tpep_pickup_datetime\")\n",
    "    drop_col = cols.get(\"tpep_dropoff_datetime\")\n",
    "    pu_col   = cols.get(\"PULocationID\")\n",
    "    do_col   = cols.get(\"DOLocationID\")\n",
    "    pc_col   = cols.get(\"passenger_count\")\n",
    "    fare_col = cols.get(\"fare_amount\")\n",
    "\n",
    "    n = batch.num_rows\n",
    "    docs = []\n",
    "    for i in range(n):\n",
    "        # Extract values safely (handle nulls)\n",
    "        # New:\n",
    "        def get_value(col, i):\n",
    "            # Returns None automatically for nulls\n",
    "            if col is None:\n",
    "                return None\n",
    "            return col[i].as_py()\n",
    "        \n",
    "        def to_dt_utc(x):\n",
    "            # Normalize to timezone-aware UTC\n",
    "            if x is None:\n",
    "                return None\n",
    "            # x can be a Python datetime (from Arrow) or a string\n",
    "            from datetime import datetime, timezone\n",
    "            if isinstance(x, str):\n",
    "                from dateutil import parser as dtparse\n",
    "                dt = dtparse.parse(x)\n",
    "            else:\n",
    "                dt = x\n",
    "            if getattr(dt, \"tzinfo\", None) is None:\n",
    "                dt = dt.replace(tzinfo=timezone.utc)\n",
    "            else:\n",
    "                dt = dt.astimezone(timezone.utc)\n",
    "            return dt\n",
    "\n",
    "        doc = {\n",
    "            \"pickup\": {\n",
    "                \"time\": to_dt_utc(get_value(pick_col, i)),\n",
    "                \"location_id\": get_value(pu_col, i)\n",
    "            },\n",
    "            \"dropoff\": {\n",
    "                \"time\": to_dt_utc(get_value(drop_col, i)),\n",
    "                \"location_id\": get_value(do_col, i)\n",
    "            },\n",
    "            \"passenger_count\": get_value(pc_col, i),\n",
    "            \"fare_amount\": get_value(fare_col, i),\n",
    "        }\n",
    "\n",
    "\n",
    "        # Optional: attach partition info for traceability (e.g., year/month)\n",
    "        if partition_values:\n",
    "            doc.setdefault(\"meta\", {})[\"partition\"] = partition_values\n",
    "\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "def main():\n",
    "    dataset = ds.dataset(PARQUET_ROOT, format=\"parquet\", partitioning=\"hive\")  # understands year=2019/month=01/...\n",
    "\n",
    "    # Iterate by fragments (files/partitions) → smaller Arrow tables → record batches\n",
    "    for fragment in dataset.get_fragments():\n",
    "        # Extract year/month from a path like .../year=2019/month=01/....\n",
    "        m = re.search(r\"year=(\\d{4})/month=(\\d{2})\", fragment.path)\n",
    "        part_vals = {\"year\": m.group(1), \"month\": m.group(2)} if m else None\n",
    "    \n",
    "        table = fragment.to_table()  # or .to_table(columns=[...]) to project fewer cols\n",
    "        for batch in table.to_batches(max_chunksize=BATCH_SIZE):\n",
    "            docs = batch_to_docs(batch, partition_values=part_vals)\n",
    "            if docs:\n",
    "                col.bulk_write([InsertOne(d) for d in docs], ordered=False)\n",
    "                print(f\"Inserted {len(docs)} docs from {fragment.path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5344f9-ebe7-4481-90b4-bb91fa34294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, glob, itertools\n",
    "\n",
    "# BASES = [\n",
    "#     \"/home/jovyan/work/data\",\n",
    "#     \"/data/taxi\",                          # in case you also mounted this\n",
    "# ]\n",
    "\n",
    "# print(\"Checking common bases…\")\n",
    "# for b in BASES:\n",
    "#     print(b, \"exists?\" , os.path.exists(b))\n",
    "\n",
    "# # Find any parquet files (limit output)\n",
    "# candidates = []\n",
    "# for base in BASES:\n",
    "#     for p in itertools.islice(glob.iglob(base + \"/**/*.parquet\", recursive=True), 50):\n",
    "#         candidates.append(p)\n",
    "# print(f\"\\nFound {len(candidates)} parquet files (showing up to 50):\")\n",
    "# for p in candidates[:50]:\n",
    "#     print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3eb436-4b36-45a0-9863-91690fba7ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting…\n",
      "Using manual window: 2019-01-01T00:00:00+00:00 → 2019-01-02T00:00:00+00:00\n",
      "\n",
      "=== Month-by-month Backfill (by pickup.time) ===\n",
      "time_bucket → rounds pickup.time to the hour for faster grouping\n",
      "trip_id     → synthetic unique key to prevent duplicates\n",
      "\n",
      "MonthStart(UTC)          Total |  TB_missing ->  TB_mod (    s) |  ID_missing ->  ID_mod (    s)\n",
      "2019-01-01T00:00:00+00:00    189432 |           0 ->       0 (  0.4) |      189432 ->  189432 (  7.4)\n",
      "\n",
      "Creating partial unique index on trip_id …\n",
      "⚠️  DuplicateKeyError while creating unique index. Investigate duplicates.\n",
      "Index build failed: 26362b9a-1aac-40ee-82e7-93011b855bee: Collection taxi_logs.trips ( 81f13904-2100-4505-8de9-a4728b198a32 ) :: caused by :: E11000 duplicate key error collection: taxi_logs.trips index: uniq_trip_id_partial dup key: { trip_id: \"2019-01-01T00:25:12.000Z:141:263:5\" }, full error: {'ok': 0.0, 'errmsg': 'Index build failed: 26362b9a-1aac-40ee-82e7-93011b855bee: Collection taxi_logs.trips ( 81f13904-2100-4505-8de9-a4728b198a32 ) :: caused by :: E11000 duplicate key error collection: taxi_logs.trips index: uniq_trip_id_partial dup key: { trip_id: \"2019-01-01T00:25:12.000Z:141:263:5\" }', 'code': 11000, 'codeName': 'DuplicateKey', 'keyPattern': {'trip_id': 1}, 'keyValue': {'trip_id': '2019-01-01T00:25:12.000Z:141:263:5'}}\n",
      "\n",
      "=== Summary ===\n",
      "Docs scanned (sum of month totals): 189,432\n",
      "time_bucket modified: 0\n",
      "trip_id modified:    189,432\n",
      "Elapsed: 2.0 minutes\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError, AutoReconnect\n",
    "import time\n",
    "\n",
    "# ------------ toggles ------------\n",
    "SKIP_CREATE_TIME_INDEX = True  # build at the very end instead\n",
    "USE_MANUAL_RANGE = True        # start small to see output quickly\n",
    "MANUAL_MIN = datetime(2019, 1, 1, tzinfo=timezone.utc)\n",
    "MANUAL_MAX = datetime(2019, 1, 2, tzinfo=timezone.utc)  # just 1 day\n",
    "# MANUAL_MIN = datetime(2019, 1, 1, tzinfo=timezone.utc)\n",
    "# MANUAL_MAX = datetime(2019, 2, 1, tzinfo=timezone.utc)  # one month\n",
    "CREATE_UNIQUE_INDEX = True\n",
    "# ---------------------------------\n",
    "\n",
    "MONGO_URL = \"mongodb://mongouser:mongopassword@mongodb:27017/taxi_logs?authSource=admin\"\n",
    "DB_NAME, COLL_NAME = \"taxi_logs\", \"trips\"\n",
    "\n",
    "print(\"Connecting…\", flush=True)\n",
    "client = MongoClient(\n",
    "    MONGO_URL,\n",
    "    serverSelectionTimeoutMS=60000,\n",
    "    connectTimeoutMS=60000,\n",
    "    socketTimeoutMS=0,\n",
    "    retryWrites=True,\n",
    "    retryReads=True\n",
    ")\n",
    "col = client[DB_NAME][COLL_NAME]\n",
    "\n",
    "def with_retries(fn, what, attempts=5):\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            return fn()\n",
    "        except AutoReconnect as e:\n",
    "            sleep = 1.5 * (2 ** i)\n",
    "            print(f\"⚠️  AutoReconnect during {what}. Retry {i+1}/{attempts} in {sleep:.1f}s …\", flush=True)\n",
    "            time.sleep(sleep)\n",
    "    # last try without catching to surface the error\n",
    "    return fn()\n",
    "\n",
    "def utc(dt):  # ensure tz-aware\n",
    "    return dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)\n",
    "\n",
    "def month_bounds(start_dt, end_dt):\n",
    "    start = utc(datetime(start_dt.year, start_dt.month, 1))\n",
    "    cur = start\n",
    "    while cur < end_dt:\n",
    "        nxt = utc(datetime(cur.year + 1, 1, 1)) if cur.month == 12 else utc(datetime(cur.year, cur.month + 1, 1))\n",
    "        yield cur, min(nxt, end_dt)\n",
    "        cur = nxt\n",
    "\n",
    "def get_time_range():\n",
    "    print(\"Computing min/max pickup.time (can be slow on large collections)…\", flush=True)\n",
    "    pipe = [\n",
    "        {\"$match\": {\"pickup.time\": {\"$type\": \"date\"}}},\n",
    "        {\"$group\": {\"_id\": None, \"minT\": {\"$min\": \"$pickup.time\"}, \"maxT\": {\"$max\": \"$pickup.time\"}}}\n",
    "    ]\n",
    "    agg = with_retries(lambda: list(col.aggregate(pipe, allowDiskUse=True)), \"aggregate(min/max)\")\n",
    "    if not agg or agg[0][\"minT\"] is None or agg[0][\"maxT\"] is None:\n",
    "        raise SystemExit(\"No documents with pickup.time as a date were found.\")\n",
    "    # make max exclusive by adding 1 day to ensure final month closes\n",
    "    return agg[0][\"minT\"], agg[0][\"maxT\"] + timedelta(days=1)\n",
    "\n",
    "def count_month(start, end):\n",
    "    return with_retries(lambda: col.count_documents({\"pickup.time\": {\"$gte\": start, \"$lt\": end}}),\n",
    "                        \"count_documents(month)\")\n",
    "\n",
    "def count_missing_time_bucket(start, end):\n",
    "    q = {\"pickup.time\": {\"$gte\": start, \"$lt\": end, \"$type\": \"date\"},\n",
    "         \"meta.time_bucket\": {\"$exists\": False}}\n",
    "    return with_retries(lambda: col.count_documents(q), \"count TB_missing\")\n",
    "\n",
    "def count_missing_trip_id(start, end):\n",
    "    q = {\"pickup.time\": {\"$gte\": start, \"$lt\": end}, \"trip_id\": {\"$exists\": False}}\n",
    "    return with_retries(lambda: col.count_documents(q), \"count ID_missing\")\n",
    "\n",
    "def backfill_time_bucket(start, end):\n",
    "    filt = {\"pickup.time\": {\"$gte\": start, \"$lt\": end, \"$type\": \"date\"},\n",
    "            \"meta.time_bucket\": {\"$exists\": False}}\n",
    "    t0 = time.perf_counter()\n",
    "    res = with_retries(lambda: col.update_many(\n",
    "        filt,\n",
    "        [{\"$set\": {\"meta.time_bucket\": {\"$dateTrunc\": {\"date\": \"$pickup.time\", \"unit\": \"hour\"}}}}]\n",
    "    ), \"update_many(time_bucket)\")\n",
    "    return res.matched_count, res.modified_count, time.perf_counter() - t0\n",
    "\n",
    "def backfill_trip_id(start, end):\n",
    "    filt = {\"pickup.time\": {\"$gte\": start, \"$lt\": end}, \"trip_id\": {\"$exists\": False}}\n",
    "    t0 = time.perf_counter()\n",
    "    res = with_retries(lambda: col.update_many(\n",
    "        filt,\n",
    "        [{\"$set\": {\"trip_id\": {\n",
    "            \"$concat\": [\n",
    "                {\"$dateToString\": {\"date\": \"$pickup.time\", \"format\": \"%Y-%m-%dT%H:%M:%S.%LZ\", \"timezone\": \"UTC\"}},\n",
    "                \":\", {\"$toString\": {\"$ifNull\": [\"$pickup.location_id\", \"na\"]}},\n",
    "                \":\", {\"$toString\": {\"$ifNull\": [\"$dropoff.location_id\", \"na\"]}},\n",
    "                \":\", {\"$toString\": {\"$ifNull\": [\"$fare_amount\", \"na\"]}}\n",
    "            ]}}}]\n",
    "    ), \"update_many(trip_id)\")\n",
    "    return res.matched_count, res.modified_count, time.perf_counter() - t0\n",
    "\n",
    "def main():\n",
    "    # 0) (Optional) defer index until the end so we see output sooner\n",
    "    if not SKIP_CREATE_TIME_INDEX:\n",
    "        print(\"Creating index on pickup.time …\", flush=True)\n",
    "        with_retries(lambda: col.create_index([(\"pickup.time\", 1)], name=\"idx_pickup_time\"),\n",
    "                     \"create_index(pickup.time)\")\n",
    "        print(\"Index on pickup.time created.\", flush=True)\n",
    "\n",
    "    # 1) Determine window(s)\n",
    "    if USE_MANUAL_RANGE:\n",
    "        minT, maxT = MANUAL_MIN, MANUAL_MAX\n",
    "        print(f\"Using manual window: {minT.isoformat()} → {maxT.isoformat()}\", flush=True)\n",
    "    else:\n",
    "        minT, maxT = get_time_range()\n",
    "        print(f\"Discovered window:   {minT.isoformat()} → {maxT.isoformat()}\", flush=True)\n",
    "\n",
    "    print(\"\\n=== Month-by-month Backfill (by pickup.time) ===\", flush=True)\n",
    "    print(\"time_bucket → rounds pickup.time to the hour for faster grouping\", flush=True)\n",
    "    print(\"trip_id     → synthetic unique key to prevent duplicates\\n\", flush=True)\n",
    "    print(f\"{'MonthStart(UTC)':<20}{'Total':>10} | \"\n",
    "          f\"{'TB_missing':>11} -> {'TB_mod':>7} ({'s':>5}) | \"\n",
    "          f\"{'ID_missing':>11} -> {'ID_mod':>7} ({'s':>5})\", flush=True)\n",
    "\n",
    "    grand_total = grand_tb_mod = grand_id_mod = 0\n",
    "    job_t0 = time.perf_counter()\n",
    "\n",
    "    for mstart, mend in month_bounds(minT, maxT):\n",
    "        total = count_month(mstart, mend)\n",
    "        grand_total += total\n",
    "        if total == 0:\n",
    "            print(f\"{mstart.isoformat():<20}{0:>10} | {0:>11} -> {0:>7} ({0:>5}) | {0:>11} -> {0:>7} ({0:>5})\",\n",
    "                  flush=True)\n",
    "            continue\n",
    "\n",
    "        tb_miss = count_missing_time_bucket(mstart, mend)\n",
    "        tb_matched, tb_mod, tb_s = backfill_time_bucket(mstart, mend)\n",
    "\n",
    "        id_miss = count_missing_trip_id(mstart, mend)\n",
    "        id_matched, id_mod, id_s = backfill_trip_id(mstart, mend)\n",
    "\n",
    "        grand_tb_mod += tb_mod\n",
    "        grand_id_mod += id_mod\n",
    "\n",
    "        print(f\"{mstart.isoformat():<20}{total:>10} | \"\n",
    "              f\"{tb_miss:>11} -> {tb_mod:>7} ({tb_s:>5.1f}) | \"\n",
    "              f\"{id_miss:>11} -> {id_mod:>7} ({id_s:>5.1f})\",\n",
    "              flush=True)\n",
    "\n",
    "    # 2) Build unique index at the end (with retry)\n",
    "    if CREATE_UNIQUE_INDEX:\n",
    "        print(\"\\nCreating partial unique index on trip_id …\", flush=True)\n",
    "        try:\n",
    "            with_retries(lambda: col.create_index(\n",
    "                [(\"trip_id\", 1)],\n",
    "                unique=True,\n",
    "                partialFilterExpression={\"trip_id\": {\"$type\": \"string\"}},\n",
    "                name=\"uniq_trip_id_partial\"\n",
    "            ), \"create_index(trip_id unique)\")\n",
    "            print(\"Unique index created on trip_id (partial).\", flush=True)\n",
    "        except DuplicateKeyError as e:\n",
    "            print(\"⚠️  DuplicateKeyError while creating unique index. Investigate duplicates.\", flush=True)\n",
    "            print(e, flush=True)\n",
    "\n",
    "    elapsed = time.perf_counter() - job_t0\n",
    "    print(\"\\n=== Summary ===\", flush=True)\n",
    "    print(f\"Docs scanned (sum of month totals): {grand_total:,}\", flush=True)\n",
    "    print(f\"time_bucket modified: {grand_tb_mod:,}\", flush=True)\n",
    "    print(f\"trip_id modified:    {grand_id_mod:,}\", flush=True)\n",
    "    print(f\"Elapsed: {elapsed/60:.1f} minutes\", flush=True)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f6f1c-aec6-4639-a55b-0ac9c910b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[█████-------------------------] 1/6 pickup.time"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient, ASCENDING, GEOSPHERE\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def progress_bar(step, total_steps, label=\"\"):\n",
    "    bar_len = 30\n",
    "    filled = int(round(bar_len * step / float(total_steps)))\n",
    "    bar = \"█\" * filled + \"-\" * (bar_len - filled)\n",
    "    sys.stdout.write(f\"\\r[{bar}] {step}/{total_steps} {label}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "client = MongoClient(\"mongodb://mongouser:mongopassword@mongodb:27017/taxi_logs?authSource=admin\")\n",
    "c = client.taxi_logs.trips\n",
    "\n",
    "indexes = [\n",
    "    ((\"pickup.time\", ASCENDING), \"pickup.time\"),\n",
    "    ((\"dropoff.time\", ASCENDING), \"dropoff.time\"),\n",
    "    ((\"pickup.location_id\", ASCENDING), \"pickup.location_id + pickup.time\"),\n",
    "    ((\"dropoff.location_id\", ASCENDING), \"dropoff.location_id + dropoff.time\"),\n",
    "    ((\"meta.time_bucket\", ASCENDING), \"meta.time_bucket\"),\n",
    "    ((\"pickup.loc\", GEOSPHERE), \"pickup.loc (geospatial)\")\n",
    "]\n",
    "\n",
    "total = len(indexes)\n",
    "for i, (spec, label) in enumerate(indexes, start=1):\n",
    "    # Mongo expects a list of tuples for compound indexes\n",
    "    if isinstance(spec, tuple):\n",
    "        spec = [spec]\n",
    "\n",
    "    c.create_index(spec)\n",
    "    progress_bar(i, total, label)\n",
    "    time.sleep(0.5)  # just to make the bar visible (remove in real run)\n",
    "\n",
    "print(\"\\nAll indexes created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34206578-7ae7-4932-a4a6-8fe4d25a6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all indexes\n",
    "for ix in c.list_indexes():\n",
    "    print(ix)\n",
    "\n",
    "# Quick sanity: ensure a query uses your index\n",
    "plan = c.find(\n",
    "    {\"pickup.location_id\": 132, \"pickup.time\": {\"$gte\": ISODate(\"2019-01-01T08:00:00Z\")}},\n",
    "    {\"_id\": 0}\n",
    ").explain(\"executionStats\")\n",
    "print(plan[\"queryPlanner\"][\"winningPlan\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f4ce6-f1ce-4b89-8569-2d6d184225fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 — Query & Validate\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# ---- Connect ----\n",
    "MONGO_URL = \"mongodb://mongouser:mongopassword@mongodb:27017/taxi_logs?authSource=admin\"\n",
    "client = MongoClient(MONGO_URL)\n",
    "c = client.taxi_logs.trips\n",
    "\n",
    "# ---- 1) Sanity counts vs Hive (Trips in Jan 2019, UTC) ----\n",
    "jan1 = datetime(2019, 1, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "feb1 = datetime(2019, 2, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "\n",
    "count_jan = c.count_documents({\n",
    "    \"pickup.time\": {\"$gte\": jan1, \"$lt\": feb1}\n",
    "})\n",
    "print(\"Trips in 2019-01:\", count_jan)\n",
    "\n",
    "# ---- 2) Targeted query using index (limit 3) ----\n",
    "docs = list(\n",
    "    c.find(\n",
    "        {\n",
    "            \"pickup.location_id\": 132,\n",
    "            \"pickup.time\": {\"$gte\": jan1, \"$lt\": jan1.replace(day=2)}\n",
    "        },\n",
    "        # optional projection:\n",
    "        {\"_id\": 0, \"pickup\": 1, \"dropoff\": 1, \"fare_amount\": 1}\n",
    "    ).limit(3)\n",
    ")\n",
    "print(\"Sample docs:\", docs)\n",
    "\n",
    "# ---- 3) Check it uses your index (explain) ----\n",
    "explain = c.find(\n",
    "    {\n",
    "        \"pickup.location_id\": 132,\n",
    "        \"pickup.time\": {\"$gte\": jan1, \"$lt\": jan1.replace(day=2)}\n",
    "    },\n",
    "    {\"_id\": 0}\n",
    ").explain(\"executionStats\")\n",
    "\n",
    "# Helper to summarize plan quality\n",
    "def summarize_explain(exp):\n",
    "    qp = exp.get(\"queryPlanner\", {})\n",
    "    winning = qp.get(\"winningPlan\", {})\n",
    "    execstats = exp.get(\"executionStats\", {})\n",
    "    stage = winning.get(\"stage\", \"\")\n",
    "    # Walk nested inputStage(s) to find IXSCAN if present\n",
    "    def find_stages(node, hits=None):\n",
    "        if hits is None:\n",
    "            hits = []\n",
    "        if not isinstance(node, dict):\n",
    "            return hits\n",
    "        if node.get(\"stage\") in (\"IXSCAN\", \"FETCH\"):\n",
    "            hits.append(node.get(\"stage\"))\n",
    "        for key in (\"inputStage\", \"inputStages\", \"shards\"):\n",
    "            child = node.get(key)\n",
    "            if isinstance(child, dict):\n",
    "                find_stages(child, hits)\n",
    "            elif isinstance(child, list):\n",
    "                for ch in child:\n",
    "                    find_stages(ch, hits)\n",
    "        return hits\n",
    "    stages = find_stages(winning)\n",
    "    print(\"Winning stage:\", stage, \"| Stages seen:\", stages)\n",
    "    print(\"nReturned:\", execstats.get(\"nReturned\"))\n",
    "    print(\"totalDocsExamined:\", execstats.get(\"totalDocsExamined\"))\n",
    "    print(\"totalKeysExamined:\", execstats.get(\"totalKeysExamined\"))\n",
    "    # Rule of thumb: keysExamined ≫ 0 and docsExamined ≪ collection size, and IXSCAN present\n",
    "    if \"IXSCAN\" in stages:\n",
    "        print(\"✅ Index scan detected (good).\")\n",
    "    else:\n",
    "        print(\"⚠️ No IXSCAN detected—query may not be using your index.\")\n",
    "\n",
    "summarize_explain(explain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
