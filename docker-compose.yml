# docker-compose.yml

# version: '3.8'

services:
  # --------------------------------------------------------------------------
  # Hadoop NameNode Service
  # --------------------------------------------------------------------------
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    hostname: namenode
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - namenode-data:/hadoop/dfs/name
    environment:
    - CLUSTER_NAME=test-cluster
    - HDFS_USER=hdfs
    - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    env_file:
      - ./hadoop.env
    networks:
      - big-data-net
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 1g

  # --------------------------------------------------------------------------
  # Hadoop DataNode Service
  # --------------------------------------------------------------------------
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    hostname: datanode
    container_name: datanode
    volumes:
      - datanode-data:/hadoop/dfs/data
      # - ./core-site.xml:/opt/hadoop-3.2.1/etc/hadoop/core-site.xml:ro
      # - ./hdfs-site.xml:/opt/hadoop-3.2.1/etc/hadoop/hdfs-site.xml:ro
    environment:
      - CLUSTER_NAME=test-cluster
      - HDFS_USER=hdfs
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_datanode_hostname=datanode
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=true
    env_file:
      - ./hadoop.env
    networks:
      - big-data-net
    depends_on:
      namenode:
        condition: service_healthy
    mem_limit: 2g
  hdfs-bootstrap:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8  # has hdfs cli
    hostname: hdfs-bootstrap
    container_name: hdfs-bootstrap
    networks:
      - big-data-net
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_USER=hdfs
    env_file:
      - ./hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy
    restart: "no"
    command: >
      bash -lc '
        set -e
        hdfs dfsadmin -safemode wait
        # HDFS temp + warehouse
        hdfs dfs -mkdir -p /tmp /tmp/hive /user/hive /user/hive/warehouse
        hdfs dfs -chmod 1777 /tmp /tmp/hive
        # MR staging for user "hive"
        hdfs dfs -mkdir -p /user/hive/.staging
        hdfs dfs -chown -R hive:supergroup /user/hive
        hdfs dfs -chmod 775 /user/hive /user/hive/warehouse
        hdfs dfs -chmod 700 /user/hive/.staging
        echo "HDFS bootstrap done."
      '
  # --------------------------------------------------------------------------
  # MariaDB Service (New Hive Metastore)
  # --------------------------------------------------------------------------
  mariadb:
    image: mariadb:10.6
    hostname: mariadb
    container_name: mariadb
    ports:
      - "3306:3306"
    volumes:
      - /mnt/c/Users/harry/Desktop/工作/DE/Data-Quality-Metadata-Deployment-Practices/dq:/opt/dq:ro
      - mariadb-data:/var/lib/mysql
    environment:
      - MARIADB_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD}
      - MARIADB_DATABASE=hive_metastore
      - MARIADB_USER=hive
      - MARIADB_PASSWORD=hive_password
    networks:
      - big-data-net
    healthcheck:
      test: ["CMD", "mariadb-admin", "ping", "-h", "localhost", "-u", "root", "-p$$MARIADB_ROOT_PASSWORD"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 512m
  mariadb-bootstrap:
    image: mariadb:10.6
    container_name: mariadb-bootstrap
    depends_on:
      mariadb:
        condition: service_healthy
    environment:
      - MARIADB_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD}   # repeat here (or use an env_file)
    volumes:
      - ./docker/mariadb-init:/init:ro
    command: >
      bash -lc '
        cat /init/10_dw.sql | mysql -hmariadb -uroot -p"$MARIADB_ROOT_PASSWORD"
      '
    restart: "no"
    networks: [big-data-net]

  # --------------------------------------------------------------------------
  # Hive Service (Metastore and HiveServer2)
  # --------------------------------------------------------------------------
  hive-server:
    image: apache/hive:3.1.3
    container_name: hive-server
    hostname: hive-server
    ports:
      - "10000:10000"
      - "9083:9083"
    volumes:
      - /mnt/c/Users/harry/Desktop/工作/DE/Data-Quality-Metadata-Deployment-Practices/dq:/opt/dq:ro
      - ./hive-start.sh:/usr/local/bin/hive-start.sh:ro
      - ./hive-site.xml:/opt/hive/conf/hive-site.xml:ro
      - ./mysql-connector-j-8.4.0.jar:/opt/hive/lib/mysql-connector-j-8.4.0.jar:ro
      - ./hadoop-conf:/opt/hadoop/etc/hadoop:ro     # <- mount the whole dir
    environment:
      HIVE_CONF_DIR: /opt/hive/conf
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      HIVE_SERVER2_HEAPSIZE: 6144              # <- add this
      HADOOP_CLIENT_OPTS: "-Xmx4g"             # <- also boost client JVM heap
    networks:
      - big-data-net
    depends_on:
      namenode:
        condition: service_healthy
      mariadb:
        condition: service_healthy
      hdfs-bootstrap:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "/opt/hive/bin/beeline -u 'jdbc:hive2://127.0.0.1:10000/' -n hive -e 'show databases' >/dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 10
      start_period: 150s
    command: >
      bash -lc '
        # local scratch for MR/HS2
        mkdir -p /tmp/mapred/local /tmp/hadoop-hive /tmp/hive
        chmod -R 777 /tmp/mapred /tmp/hadoop-hive /tmp/hive
        mkdir -p /home/hive && chown -R hive:hive /home/hive
        export HOME=/tmp
        /opt/hive/bin/hiveserver2
      '
    entrypoint: ["/usr/local/bin/hive-start.sh"]
    
    

  # --------------------------------------------------------------------------
  # MongoDB Service
  # --------------------------------------------------------------------------
  mongodb:
    image: mongo:6.0
    hostname: mongodb
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongodb-data:/data/db
    environment:
      - MONGO_INITDB_ROOT_USERNAME=mongouser
      - MONGO_INITDB_ROOT_PASSWORD=mongopassword
    networks:
      - big-data-net
    healthcheck:
      test: [ "CMD", "mongosh", "--eval", "db.adminCommand('ping')", "--quiet" ]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 512m

  # --------------------------------------------------------------------------
  # JupyterLab Service
  # --------------------------------------------------------------------------
  jupyterlab:
    image: jupyter/scipy-notebook:latest
    hostname: jupyterlab
    container_name: jupyterlab
    ports:
      - "8888:8888"
    volumes:
      # Mount the local notebooks directory
      - ./notebooks:/home/jovyan/work
      # Add this line to mount your local data directory
      - ./data:/home/jovyan/work/data
      - "/mnt/c/Users/harry/Desktop/工作/DE/Data-Quality-Metadata-Deployment-Practices/dq:/home/jovyan/work/dq_ext"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - GRANT_SUDO=yes
      - NB_UID=1000
      - NB_GID=100
      - HADOOP_NAMENODE_HOST=namenode
      - HIVE_SERVER_HOST=hive-server
      - HIVE_SERVER_PORT=10000
      - MARIADB_HOST=mariadb
      - MARIADB_PORT=3306
      - MARIADB_USER=hive
      - MARIADB_PASSWORD=hive_password
      - MONGODB_HOST=mongodb
      - MONGODB_PORT=27017
      - MONGODB_USER=mongouser
      - MONGODB_PASSWORD=mongopassword
    networks:
      - big-data-net
    depends_on:
      namenode:
        condition: service_healthy
      hive-server:
        condition: service_healthy
      mariadb:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    mem_limit: 1.5g
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.allow_origin='*'
  airflow-db:
    image: postgres:14
    container_name: airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports: ["5434:5432"]
    networks: [big-data-net]
    healthcheck:
      # NOTE: use $$ so compose passes $ through to the container
      test: ["CMD-SHELL","pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 40

  airflow-webserver:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    container_name: airflow-webserver
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW_UID: "${AIRFLOW_UID}"
      # Connections
      AIRFLOW_CONN_HIVE_DEFAULT: "hive://hive-server:10000/"
      # FIX B: match your MariaDB creds & DB (see below)
      AIRFLOW_CONN_MARIADB_DEFAULT: "mysql://root:your_root_password@mariadb:3306/analytics"
      # Optional lineage
      AIRFLOW__OPENLINEAGE__NAMESPACE: "dev"
      AIRFLOW__OPENLINEAGE__TRANSPORT: '{"type":"http","url":"http://marquez:5000"}'
    volumes:
      - "/mnt/c/Users/harry/Desktop/工作/DE/Data-Quality-Metadata-Deployment-Practices/dags:/opt/airflow/dags"
      - ./dq:/opt/airflow/dq
      - airflow_logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    group_add:
      - "${DOCKER_GID}"
    ports: ["8081:8080"]
    command: webserver
    networks: [big-data-net]

  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    container_name: airflow-scheduler
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW_UID: "${AIRFLOW_UID}"
      AIRFLOW_CONN_HIVE_DEFAULT: "hive://hive-server:10000/"
      AIRFLOW_CONN_MARIADB_DEFAULT: "mysql://root:your_root_password@mariadb:3306/analytics"
      AIRFLOW__OPENLINEAGE__NAMESPACE: "dev"
      AIRFLOW__OPENLINEAGE__TRANSPORT: '{"type":"http","url":"http://marquez:5000"}'
    group_add:
      - "${DOCKER_GID}"      
    volumes:
      - /mnt/c/Users/harry/Desktop/工作/DE/Data-Quality-Metadata-Deployment-Practices/dags:/opt/airflow/dags
      - ./dq:/opt/airflow/dq
      - airflow_logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    networks: [big-data-net]
  airflow-init:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    container_name: airflow-init
    depends_on:
      airflow-db:
        condition: service_healthy
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: "${AIRFLOW__CORE__FERNET_KEY}"
      _AIRFLOW_WWW_USER_USERNAME: "${_AIRFLOW_WWW_USER_USERNAME}"
      _AIRFLOW_WWW_USER_PASSWORD: "${_AIRFLOW_WWW_USER_PASSWORD}"
      AIRFLOW_CONN_HIVE_DEFAULT: "${AIRFLOW_CONN_HIVE_DEFAULT}"
      AIRFLOW_CONN_MARIADB_DEFAULT: "${AIRFLOW_CONN_MARIADB_DEFAULT}"
    command: >
      bash -lc "
        set -euo pipefail
        echo '==> DB migrate'
        airflow db migrate

        echo '==> Ensure admin user'
        if airflow users list --output json | python -c \"import sys,json,os; u=os.environ.get('_AIRFLOW_WWW_USER_USERNAME','admin'); sys.exit(0 if any(x.get('username')==u for x in json.load(sys.stdin)) else 1)\"; then
          echo 'User exists'
        else
          airflow users create --username \"$_AIRFLOW_WWW_USER_USERNAME\" --password \"$_AIRFLOW_WWW_USER_PASSWORD\" --firstname Admin --lastname User --role Admin --email admin@example.com
        fi

        echo '==> Ensure connections'
        airflow connections get hive_default >/dev/null 2>&1 && airflow connections delete hive_default || true
        airflow connections add hive_default --conn-uri \"$AIRFLOW_CONN_HIVE_DEFAULT\"

        airflow connections get mariadb_default >/dev/null 2>&1 && airflow connections delete mariadb_default || true
        airflow connections add mariadb_default --conn-uri \"$AIRFLOW_CONN_MARIADB_DEFAULT\"
      "
    restart: "no"
    networks: [big-data-net]

  marquez-db:
    image: postgres:14
    container_name: marquez-db
    env_file: [.env]
    ports: ["5433:5432"]         # host:container; avoid conflicts with local Postgres
    networks:
      big-data-net:
        aliases:
          - postgres    # so 'postgres:5432' resolves inside the compose network
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
      interval: 5s
      timeout: 3s
      retries: 30

  marquez:
    image: marquezproject/marquez:0.50.0
    container_name: marquez
    depends_on:
      marquez-db:
        condition: service_healthy
    env_file: [.env]
    ports:
      - "5000:5000"   # API
      - "5001:5001"   # admin (healthcheck/metrics)
    networks: [big-data-net]
    healthcheck:
      test: ["CMD","curl","-sf","http://localhost:5000/api/v1/health"]
      interval: 5s
      timeout: 3s
      retries: 40

  marquez-web:
    image: marquezproject/marquez-web:0.50.0   # pin a matching tag
    environment:
      - WEB_PORT=3000
      - MARQUEZ_BASE_URL=http://marquez:5000   # keep both styles for safety
      - MARQUEZ_HOST=marquez
      - MARQUEZ_PORT=5000
    depends_on:
      - marquez
        # condition: service_healthy
    ports:
      - "3000:3000"   # change to 3001:3000 if 3000 is busy
    networks:
      - big-data-net

# --------------------------------------------------------------------------
# Docker Volumes for Persistent Data
# --------------------------------------------------------------------------
volumes:
  namenode-data:
  datanode-data:
  mariadb-data:
  mongodb-data:
  airflow_logs:


# --------------------------------------------------------------------------
# Docker Network for Inter-Service Communication
# --------------------------------------------------------------------------
networks:
  big-data-net:
    driver: bridge
