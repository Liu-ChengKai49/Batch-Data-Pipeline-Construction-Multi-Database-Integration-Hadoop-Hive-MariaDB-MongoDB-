{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f323de23-810d-4278-99f1-81866ce58990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.0.tar.gz (434.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 MB\u001b[0m \u001b[31m866.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.9 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741238 sha256=e727ab6f235a441c9dbdf09132b23ab54cb312c068863b835d89f6e82ee97fe0\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/91/e4/c1/3c917d48563ae77204dd185aa3da90da6a1a5526565296dadf\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c37ed76-2e4d-495e-a8be-65710c9956a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.28\" 2025-07-15\n",
      "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
      "\u001b[33mWARNING: Skipping pyspark as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping py4j as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.1\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m176.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=eee6a65e9ecc8973c66f12f729e120ed8a98f37fd97ea7a8990ac86183dff188\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gu6x4u4_/wheels/95/13/41/f7f135ee114175605fb4f0a89e7389f3742aa6c1e1a5bcb657\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
      "PySpark: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "# import subprocess, sys\n",
    "\n",
    "# # Java should already be present from your apt step:\n",
    "# subprocess.run([\"java\", \"-version\"], check=True)\n",
    "\n",
    "# # Install PySpark to the jovyan user env\n",
    "# subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyspark\", \"py4j\"], check=True)\n",
    "# subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "#                 \"pyspark==3.5.1\", \"py4j==0.10.9.7\"], check=True)\n",
    "# import pyspark\n",
    "# print(\"PySpark:\", pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0239019-d6df-4ca2-9ea1-9bb202263026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/24 07:12:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"  # apt's path\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = (SparkSession.builder\n",
    "#          .master(\"local[*]\")\n",
    "#          .config(\"spark.ui.showConsoleProgress\",\"false\")\n",
    "#          .getOrCreate())\n",
    "# spark.range(5).show()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6676fb-4d5a-4215-848d-4717429de221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # MinIO creds (fallback to defaults if not set in env)\n",
    "# MINIO_ACCESS_KEY = os.getenv(\"MINIO_ROOT_USER\", \"minioadmin\")\n",
    "# MINIO_SECRET_KEY = os.getenv(\"MINIO_ROOT_PASSWORD\", \"minioadmin\")\n",
    "\n",
    "# spark = (\n",
    "#     SparkSession.builder\n",
    "#     .appName(\"sensor-dq-notebook\")\n",
    "#     .master(\"local[*]\")  # or .master(\"spark://spark:7077\") if you want to use the Spark service\n",
    "#     # Kafka + S3A bundles\n",
    "#     .config(\n",
    "#         \"spark.jars.packages\",\n",
    "#         \",\".join([\n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\",\n",
    "#             \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "#             \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "#         ])\n",
    "#     )\n",
    "#     # S3A / MinIO\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "#     # Optional: quieter logs\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# print(\"Spark:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95211763-99a2-423d-8e68-1e2fa14d61b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/24 07:12:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-89b5bb9a-d450-4251-814e-c11a5d6be45b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/08/24 07:12:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+---+\n",
      "|n  |ts |\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+-----------------------+\n",
      "|n  |ts                     |\n",
      "+---+-----------------------+\n",
      "|0  |2025-08-24 07:12:12.833|\n",
      "+---+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---+-----------------------+\n",
      "|n  |ts                     |\n",
      "+---+-----------------------+\n",
      "|1  |2025-08-24 07:12:13.729|\n",
      "+---+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+---+-----------------------+\n",
      "|n  |ts                     |\n",
      "+---+-----------------------+\n",
      "|2  |2025-08-24 07:12:15.059|\n",
      "+---+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+---+-----------------------+\n",
      "|n  |ts                     |\n",
      "+---+-----------------------+\n",
      "|3  |2025-08-24 07:12:15.516|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# test_stream = (\n",
    "#     spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "#     .select(F.col(\"value\").alias(\"n\"), F.current_timestamp().alias(\"ts\"))\n",
    "# )\n",
    "\n",
    "# q = (\n",
    "#     test_stream.writeStream.format(\"console\")\n",
    "#     .outputMode(\"append\")\n",
    "#     .option(\"truncate\", \"false\")\n",
    "#     .start()\n",
    "# )\n",
    "# q.processAllAvailable()\n",
    "# q.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7753c8eb-cc28-4b7a-a835-fa7ba7ab180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 3.5.1\n",
      "Packages: org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# MINIO_ACCESS_KEY = os.getenv(\"MINIO_ROOT_USER\", \"minioadmin\")\n",
    "# MINIO_SECRET_KEY = os.getenv(\"MINIO_ROOT_PASSWORD\", \"minioadmin\")\n",
    "\n",
    "# spark = (\n",
    "#     SparkSession.builder\n",
    "#     .appName(\"sensor-dq-notebook\")\n",
    "#     .master(\"local[*]\")  # keep local[*] for notebook\n",
    "#     .config(\n",
    "#         \"spark.jars.packages\",\n",
    "#         \",\".join([\n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\",\n",
    "#             \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "#             \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "#         ])\n",
    "#     )\n",
    "#     # MinIO / S3A\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# print(\"Spark\", spark.version)\n",
    "# print(\"Packages:\", spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58e55fd-d121-448d-b7d3-a6a58810f59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b3141bb1-1837-4402-9559-1b98a9de2e5f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 651ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b3141bb1-1837-4402-9559-1b98a9de2e5f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/11ms)\n",
      "25/08/24 07:55:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 3.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ROOT_USER\", \"minio\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_ROOT_PASSWORD\", \"minio12345678\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"sensor-dq-notebook\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \",\".join([\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "        ])\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f3282a4-1095-44a6-a091-7608888d6793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar',\n",
      " '/home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar',\n",
      " '/home/jovyan/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar',\n",
      " '/home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar',\n",
      " '/home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar']\n"
     ]
    }
   ],
   "source": [
    "import glob, os, pprint\n",
    "jar_dir = os.path.expanduser(\"~/.ivy2/jars\")\n",
    "found = sorted(glob.glob(os.path.join(jar_dir, \"*kafka*jar\"))) + \\\n",
    "        sorted(glob.glob(os.path.join(jar_dir, \"*hadoop-aws*jar\"))) + \\\n",
    "        sorted(glob.glob(os.path.join(jar_dir, \"*aws-java-sdk-bundle*jar\")))\n",
    "pprint.pp(found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa6d2fc7-439a-4eab-94ef-a9d0b4e5e345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/24 14:03:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/24 14:03:24 WARN StreamingQueryManager: Stopping existing streaming query [id=34d92474-400c-48b5-939a-bed4a42f3b87, runId=2f01f5eb-b870-403d-8ffd-42f7389ae9c6], as a new run is being started.\n",
      "25/08/24 14:03:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/24 14:03:24 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/08/24 14:03:25 WARN StreamingQueryManager: Stopping existing streaming query [id=6e1a7a85-5df5-4bd1-a569-34ab181e1b6b, runId=3b81cd78-8aed-420b-a4e7-f748774fd6a8], as a new run is being started.\n",
      "25/08/24 14:03:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/24 14:03:25 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/08/24 14:03:25 WARN StreamingQueryManager: Stopping existing streaming query [id=2888fd29-607f-464e-994b-9c14050dfef3, runId=f7d238e8-db90-416d-ad51-5fe29f9f39a8], as a new run is being started.\n",
      "25/08/24 14:03:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/24 14:03:25 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/08/24 14:03:25 WARN StreamingQueryManager: Stopping existing streaming query [id=8df5cac2-c0f6-4c63-8773-a8e5eba8fc87, runId=e3452921-f9b6-41c6-af54-ba3b7cf75ee8], as a new run is being started.\n",
      "25/08/24 14:03:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/24 14:03:26 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/08/24 14:03:26 WARN StreamingQueryManager: Stopping existing streaming query [id=ae462a92-c825-4e38-bb9f-e2d738cef1eb, runId=52fd6529-fc45-4a2b-a860-085fd2ee5dea], as a new run is being started.\n",
      "25/08/24 14:03:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/24 14:03:26 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/08/24 14:03:26 WARN StreamingQueryManager: Stopping existing streaming query [id=7c2e72b0-f218-4f1d-a6e0-50bae75b2d65, runId=f5f5d63b-aaae-41a9-9f63-75636c53d008], as a new run is being started.\n",
      "25/08/24 14:03:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/24 14:03:27 WARN StreamingQueryManager: Stopping existing streaming query [id=3e5428c7-8763-4a3e-bbfc-1620fd214b5d, runId=a1456045-5e87-4e62-b114-7f562c34b6f1], as a new run is being started.\n",
      "25/08/24 14:03:27 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/08/24 14:03:27 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "# /app/streaming/job.py\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "import os\n",
    "\n",
    "# --- SparkSession with MinIO (S3A) ---\n",
    "MINIO_ENDPOINT = os.getenv(\"S3_ENDPOINT\", \"http://minio:9000\")\n",
    "ACCESS_KEY     = os.getenv(\"AWS_ACCESS_KEY_ID\", os.getenv(\"MINIO_ROOT_USER\", \"minio\"))\n",
    "SECRET_KEY     = os.getenv(\"AWS_SECRET_ACCESS_KEY\", os.getenv(\"MINIO_ROOT_PASSWORD\", \"minioadmin\"))\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"sensor-stream-dq\")\n",
    "    # align jar versions to your Spark/PySpark (adjust if you're on 4.0.0)\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY)\n",
    "    .getOrCreate())\n",
    "\n",
    "# --- Incoming schema from Kafka JSON ---\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"device_id\",     T.StringType(),  False),\n",
    "    T.StructField(\"ts\",            T.TimestampType(), False),\n",
    "    T.StructField(\"temperature_c\", T.DoubleType(),  True),\n",
    "    T.StructField(\"vibration_g\",   T.DoubleType(),  True),\n",
    "    T.StructField(\"status\",        T.StringType(),  True),\n",
    "    T.StructField(\"site\",          T.StringType(),  True),\n",
    "    T.StructField(\"line\",          T.StringType(),  True),\n",
    "])\n",
    "\n",
    "# --- Read Kafka ---\n",
    "raw = (spark.readStream.format(\"kafka\")\n",
    "       .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "       .option(\"subscribe\", \"sensor_events\")\n",
    "       .option(\"startingOffsets\", \"latest\")\n",
    "       .option(\"maxOffsetsPerTrigger\", 5000)\n",
    "       .load())\n",
    "\n",
    "parsed = (raw.select(F.col(\"value\").cast(\"string\").alias(\"json\"),\n",
    "                     F.col(\"timestamp\").alias(\"ingest_ts\"))\n",
    "              .select(F.from_json(\"json\", schema, {\"allowComments\": \"false\"}).alias(\"r\"),\n",
    "                      \"json\", \"ingest_ts\")\n",
    "              .selectExpr(\"r.*\", \"json as raw_json\", \"ingest_ts\"))\n",
    "\n",
    "# Normalize + watermark\n",
    "events = (parsed\n",
    "          .withColumn(\"event_ts\", F.col(\"ts\"))\n",
    "          .drop(\"ts\")\n",
    "          .withWatermark(\"event_ts\", \"10 minutes\"))\n",
    "\n",
    "# Data quality flags\n",
    "bad_json         = parsed.filter(F.col(\"r\").isNull())  # parse failed\n",
    "missing_key      = events.filter(F.col(\"device_id\").isNull() | F.col(\"event_ts\").isNull())\n",
    "out_of_range_tmp = events.filter((F.col(\"temperature_c\") < -50) | (F.col(\"temperature_c\") > 150))\n",
    "bad_vibration    = events.filter(F.col(\"vibration_g\").isNotNull() & (F.col(\"vibration_g\") < 0))\n",
    "bad_status       = events.filter(~F.col(\"status\").isin(\"OK\",\"WARN\",\"FAIL\"))\n",
    "\n",
    "# Late-ish data approximation (not true watermark eviction)\n",
    "late_cutoff_min = F.lit(10)\n",
    "late_data = events.filter((F.unix_timestamp(F.current_timestamp()) - F.unix_timestamp(\"event_ts\"))/60.0 > late_cutoff_min)\n",
    "\n",
    "# Clean stream + add the **partition column** here\n",
    "clean = (events\n",
    "         .dropDuplicates([\"device_id\",\"event_ts\"])\n",
    "         .filter(~((F.col(\"temperature_c\") < -50) | (F.col(\"temperature_c\") > 150)))\n",
    "         .filter(~(F.col(\"device_id\").isNull() | F.col(\"event_ts\").isNull()))\n",
    "         .filter(~(F.col(\"vibration_g\").isNotNull() & (F.col(\"vibration_g\") < 0)))\n",
    "         .filter(F.col(\"status\").isin(\"OK\",\"WARN\",\"FAIL\") | F.col(\"status\").isNull())\n",
    "         .withColumn(\"event_date\", F.to_date(\"event_ts\"))  # <-- add partition column\n",
    ")\n",
    "\n",
    "# Batch summary (recompute bad_json from raw_json so it works on any DF)\n",
    "def dq_checks(batch_df, epoch_id: int):\n",
    "    from pyspark.sql import functions as F, types as T\n",
    "    # If 'raw_json' exists, we can recompute parse success; otherwise 0\n",
    "    if \"raw_json\" in batch_df.columns:\n",
    "        _r = F.from_json(\"raw_json\", schema)\n",
    "        b_bad_json = batch_df.filter(_r.isNull()).count()\n",
    "    else:\n",
    "        b_bad_json = 0\n",
    "\n",
    "    b_missing_key      = batch_df.filter(F.col(\"device_id\").isNull() | F.col(\"event_ts\").isNull()).count()\n",
    "    b_out_of_range_tmp = batch_df.filter((F.col(\"temperature_c\") < -50) | (F.col(\"temperature_c\") > 150)).count()\n",
    "    b_bad_vibration    = batch_df.filter(F.col(\"vibration_g\").isNotNull() & (F.col(\"vibration_g\") < 0)).count()\n",
    "    b_bad_status       = batch_df.filter(~F.col(\"status\").isin(\"OK\",\"WARN\",\"FAIL\")).count()\n",
    "    rows               = batch_df.count()\n",
    "\n",
    "    summary = batch_df.sql_ctx.createDataFrame(\n",
    "        [(int(epoch_id), int(rows), int(b_bad_json), int(b_missing_key),\n",
    "          int(b_out_of_range_tmp), int(b_bad_vibration), int(b_bad_status))],\n",
    "        schema=\"epoch_id INT, rows BIGINT, bad_json BIGINT, missing_key BIGINT, out_of_range_temp BIGINT, bad_vibration BIGINT, bad_status BIGINT\"\n",
    "    )\n",
    "\n",
    "    (summary.withColumn(\"ts\", F.current_timestamp())\n",
    "            .withColumn(\"date\", F.to_date(\"ts\"))\n",
    "            .write.mode(\"append\").partitionBy(\"date\")\n",
    "            .json(\"s3a://rt-stream/dq/streaming_summary\"))\n",
    "\n",
    "# --- Sinks ---\n",
    "CHK_ROOT = \"s3a://rt-stream/checkpoints\"\n",
    "\n",
    "# 1) Curated bronze (partition by the column that exists)\n",
    "clean_q = (clean.writeStream\n",
    "           .format(\"parquet\")\n",
    "           .option(\"path\", \"s3a://rt-stream/curated/events\")\n",
    "           .option(\"checkpointLocation\", f\"{CHK_ROOT}/curated\")\n",
    "           .partitionBy(\"event_date\")   # <-- was \"date\"; fix to existing column\n",
    "           .outputMode(\"append\")\n",
    "           .start())\n",
    "\n",
    "# 2) Quarantine writers (each with its checkpoint)\n",
    "def write_quarantine(df, path, chk):\n",
    "    return (df.withColumn(\"date\", F.to_date(F.current_timestamp()))\n",
    "              .writeStream\n",
    "              .format(\"parquet\")\n",
    "              .option(\"path\", path)\n",
    "              .option(\"checkpointLocation\", chk)\n",
    "              .partitionBy(\"date\")\n",
    "              .outputMode(\"append\")\n",
    "              .start())\n",
    "\n",
    "q1 = write_quarantine(bad_json,         \"s3a://rt-stream/dq/quarantine/corrupt_json\",  f\"{CHK_ROOT}/dq/corrupt_json\")\n",
    "q2 = write_quarantine(missing_key,      \"s3a://rt-stream/dq/quarantine/missing_key\",   f\"{CHK_ROOT}/dq/missing_key\")\n",
    "q3 = write_quarantine(out_of_range_tmp, \"s3a://rt-stream/dq/quarantine/invalid_temp\",  f\"{CHK_ROOT}/dq/invalid_temp\")\n",
    "q4 = write_quarantine(bad_vibration,    \"s3a://rt-stream/dq/quarantine/invalid_vibe\",  f\"{CHK_ROOT}/dq/invalid_vibe\")\n",
    "q5 = write_quarantine(bad_status,       \"s3a://rt-stream/dq/quarantine/invalid_status\",f\"{CHK_ROOT}/dq/invalid_status\")\n",
    "\n",
    "# 3) Summary (use events so we have raw_json available)\n",
    "dq_q = (events.writeStream\n",
    "        .foreachBatch(dq_checks)\n",
    "        .option(\"checkpointLocation\", f\"{CHK_ROOT}/dq/summary\")\n",
    "        .start())\n",
    "\n",
    "spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e290ba86-a11e-4f6c-ab7b-651de045428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unnamed>', '<unnamed>', '<unnamed>', '<unnamed>', '<unnamed>', '<unnamed>', '<unnamed>']\n",
      "\n",
      "--- <unnamed> ---\n",
      "isActive: True\n",
      "lastProgress: {'id': '7c2e72b0-f218-4f1d-a6e0-50bae75b2d65', 'runId': '3948440b-bdc6-41a0-8001-0507f4ddbfb7', 'name': None, 'timestamp': '2025-08-24T14:03:57.238Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 4, 'triggerExecution': 4}, 'eventTime': {'watermark': '2025-08-24T08:32:25.018Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/invalid_status]', 'numOutputRows': -1}}\n",
      "\n",
      "--- <unnamed> ---\n",
      "isActive: True\n",
      "lastProgress: {'id': '34d92474-400c-48b5-939a-bed4a42f3b87', 'runId': '30e35802-2d0e-4003-9e20-4baa96b61a00', 'name': None, 'timestamp': '2025-08-24T14:03:55.028Z', 'batchId': 9, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 2, 'triggerExecution': 2}, 'eventTime': {'watermark': '2025-08-24T08:32:25.299Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/curated/events]', 'numOutputRows': -1}}\n",
      "\n",
      "--- <unnamed> ---\n",
      "isActive: True\n",
      "lastProgress: {'id': '8df5cac2-c0f6-4c63-8773-a8e5eba8fc87', 'runId': 'c2854440-faf5-432e-924b-eebe909daf45', 'name': None, 'timestamp': '2025-08-24T14:03:56.317Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 3, 'triggerExecution': 4}, 'eventTime': {'watermark': '2025-08-23T23:55:00.000Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/invalid_temp]', 'numOutputRows': -1}}\n",
      "\n",
      "--- <unnamed> ---\n",
      "isActive: True\n",
      "lastProgress: {'id': 'ae462a92-c825-4e38-bb9f-e2d738cef1eb', 'runId': '0ffa4429-af92-416a-8b54-d999b816b8e0', 'name': None, 'timestamp': '2025-08-24T14:03:56.798Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 2, 'triggerExecution': 3}, 'eventTime': {'watermark': '1970-01-01T00:00:00.000Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/invalid_vibe]', 'numOutputRows': -1}}\n",
      "\n",
      "--- <unnamed> ---\n",
      "isActive: True\n",
      "lastProgress: {'id': '6e1a7a85-5df5-4bd1-a569-34ab181e1b6b', 'runId': '04ff9739-696c-4328-bf69-819a5630136f', 'name': None, 'timestamp': '2025-08-24T14:03:55.396Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 4, 'triggerExecution': 4}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/corrupt_json]', 'numOutputRows': -1}}\n",
      "\n",
      "--- <unnamed> ---\n",
      "isActive: True\n",
      "lastProgress: {'id': '3e5428c7-8763-4a3e-bbfc-1620fd214b5d', 'runId': '0913fd86-ee35-4214-995b-b6345266d5f2', 'name': None, 'timestamp': '2025-08-24T14:03:47.573Z', 'batchId': 6, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 2, 'triggerExecution': 2}, 'eventTime': {'watermark': '2025-08-24T08:32:25.299Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "\n",
      "--- <unnamed> ---\n",
      "isActive: True\n",
      "lastProgress: {'id': '2888fd29-607f-464e-994b-9c14050dfef3', 'runId': 'cc342b70-86f4-4240-825c-4aba50b21093', 'name': None, 'timestamp': '2025-08-24T14:03:55.941Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 3, 'triggerExecution': 3}, 'eventTime': {'watermark': '2025-08-24T08:32:25.299Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/missing_key]', 'numOutputRows': -1}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/24 14:04:04 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:05 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:05 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:05 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:07 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:07 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:07 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:07 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:08 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:08 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:08 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:08 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:09 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:09 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:09 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:09 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:10 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:10 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:10 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:10 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:11 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:12 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:12 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:12 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:12 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:12 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:13 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:13 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:13 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:13 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:13 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:13 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:14 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:15 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:15 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:15 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:15 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:17 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:14 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:14 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:14 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:15 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:15 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:15 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:15 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:16 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:17 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:17 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:17 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:17 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:17 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:17 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:18 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:18 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:18 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:18 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:18 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:19 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:20 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:20 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:20 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:20 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:21 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:21 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:21 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:21 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:23 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:23 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:23 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:23 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:24 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:24 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:24 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:24 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:24 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:25 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:25 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:25 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:25 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:25 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:25 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:26 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:27 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:27 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:27 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:27 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:27 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:28 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:28 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:28 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:29 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:29 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:29 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:29 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:30 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:30 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:30 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:31 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:32 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:32 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:32 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:32 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:33 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:33 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:33 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:34 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:35 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:35 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:35 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:36 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:36 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:37 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:37 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:38 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:39 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:39 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:40 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:41 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:42 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:43 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:45 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:46 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:46 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:46 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:46 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:44 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:45 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:45 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:45 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:45 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:46 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:46 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:46 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:46 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:47 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:47 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:48 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:48 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:48 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:49 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:49 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:49 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:50 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:50 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:50 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:51 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:51 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:51 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:52 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:52 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:52 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:52 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:53 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:53 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:53 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:54 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:54 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:54 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:55 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:55 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:55 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:55 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:56 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:56 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:56 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:56 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:57 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:57 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:57 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:04:58 WARN HDFSBackedStateStoreProvider: The state for version 9 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 14:05:00 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:05:02 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:05:02 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:05:02 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:05:02 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:05:03 WARN FileStreamSinkLog: Compacting took 2930 ms for compact batch 9\n",
      "25/08/24 14:05:10 WARN FileStreamSinkLog: Compacting took 2127 ms for compact batch 9\n",
      "25/08/24 14:05:11 WARN FileStreamSinkLog: Compacting took 2274 ms for compact batch 9\n",
      "25/08/24 14:06:59 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:07:00 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:07:00 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:07:00 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:07:00 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:19:50 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:19:52 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:19:52 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:19:52 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:19:52 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:21:56 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:21:57 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:21:57 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:21:57 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:21:57 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:24:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:24:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:24:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:24:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 14:24:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print([q.name or \"<unnamed>\" for q in spark.streams.active])\n",
    "for q in spark.streams.active:\n",
    "    print(\"\\n---\", q.name or \"<unnamed>\", \"---\")\n",
    "    print(\"isActive:\", q.isActive)\n",
    "    print(\"lastProgress:\", q.lastProgress)   # look for numInputRows > 0 after producer runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "371a93fc-e987-4d86-b918-7f59920c85e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------+--------+-----------+-----------------+----+------------------------+----------+\n",
      "|bad_json|bad_status|bad_vibration|epoch_id|missing_key|out_of_range_temp|rows|ts                      |date      |\n",
      "+--------+----------+-------------+--------+-----------+-----------------+----+------------------------+----------+\n",
      "|0       |281       |0            |5       |0          |0                |2997|2025-08-24T08:44:58.455Z|2025-08-24|\n",
      "|0       |1         |0            |4       |0          |0                |3   |2025-08-24T08:43:01.830Z|2025-08-24|\n",
      "|0       |1         |1            |3       |1          |1                |3   |2025-08-24T08:33:38.253Z|2025-08-24|\n",
      "|0       |0         |0            |2       |0          |0                |1   |2025-08-24T08:27:31.690Z|2025-08-24|\n",
      "|0       |1         |1            |1       |1          |1                |3   |2025-08-24T08:22:32.232Z|2025-08-24|\n",
      "|0       |0         |0            |0       |0          |0                |0   |2025-08-24T08:12:13.193Z|2025-08-24|\n",
      "+--------+----------+-------------+--------+-----------+-----------------+----+------------------------+----------+\n",
      "\n",
      "invalid_temp: 2\n",
      "missing_key : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/24 12:40:19 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listOffsets on broker 1\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2005)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listOffsets on broker 1\n",
      "25/08/24 12:40:20 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "# Summary rows\n",
    "dq = spark.read.json(\"s3a://rt-stream/dq/streaming_summary\")\n",
    "dq.orderBy(dq.epoch_id.desc()).show(10, False)\n",
    "\n",
    "# Quarantine counts\n",
    "print(\"invalid_temp:\", spark.read.parquet(\"s3a://rt-stream/dq/quarantine/invalid_temp\").count())\n",
    "print(\"missing_key :\", spark.read.parquet(\"s3a://rt-stream/dq/quarantine/missing_key\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66df2f62-77b2-4648-aa92-70f43d130c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '34d92474-400c-48b5-939a-bed4a42f3b87', 'runId': '959f1b43-aff4-4fad-afb6-013ba576214f', 'name': None, 'timestamp': '2025-08-24T08:20:44.047Z', 'batchId': 1, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 4, 'triggerExecution': 4}, 'eventTime': {'watermark': '1970-01-01T00:00:00.000Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 903, '1': 1040, '0': 1057}}, 'endOffset': {'sensor_events': {'2': 903, '1': 1040, '0': 1057}}, 'latestOffset': {'sensor_events': {'2': 903, '1': 1040, '0': 1057}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/curated/events]', 'numOutputRows': -1}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/24 08:22:02 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:02 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:02 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:03 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:05 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:05 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:05 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:05 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:06 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:06 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:06 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:06 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:09 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:09 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:09 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:09 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:10 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:10 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:10 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:10 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:12 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:13 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:14 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:15 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:16 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:19 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:17 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:18 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:19 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:20 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:20 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:20 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:20 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:21 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:21 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:22 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:23 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:23 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:24 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:24 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:24 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:24 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "25/08/24 08:22:25 WARN HDFSBackedStateStoreProvider: The state for version 1 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "25/08/24 08:22:32 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 08:24:12 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 08:24:13 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 08:24:13 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 08:24:14 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 08:27:31 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 08:27:32 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 08:27:32 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 08:27:32 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/08/24 08:27:32 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# [s.name for s in spark.streams.active], [s.status for s in spark.streams.active]\n",
    "# clean_q.isActive, dq_q.isActive\n",
    "# print(clean_q.lastProgress)   # may be None on first run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858dfc63-1be9-4deb-b4e2-bed2cf7068d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/24 07:56:15 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/08/24 07:56:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/24 07:56:19 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # make sure event_ts is a proper timestamp; fallback to ingest_ts if missing\n",
    "# clean2 = (clean\n",
    "#     .withColumn(\"event_ts\",\n",
    "#                 F.coalesce(F.col(\"event_ts\").cast(\"timestamp\"), F.col(\"ingest_ts\").cast(\"timestamp\")))\n",
    "#     .withColumn(\"event_date\", F.to_date(\"event_ts\"))            # <- new partition column\n",
    "#     # optional: also keep hour for bucketing\n",
    "#     # .withColumn(\"event_hour\", F.hour(\"event_ts\"))\n",
    "# )\n",
    "\n",
    "# CHK_ROOT = \"s3a://rt-stream/checkpoints\"\n",
    "\n",
    "# clean_q = (clean2.writeStream\n",
    "#     .format(\"parquet\")\n",
    "#     .option(\"path\", \"s3a://rt-stream/curated/events\")\n",
    "#     .option(\"checkpointLocation\", f\"{CHK_ROOT}/curated\")\n",
    "#     .partitionBy(\"event_date\")     # or: .partitionBy(\"site\",\"line\",\"event_date\")\n",
    "#     .outputMode(\"append\")\n",
    "#     .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6373af5f-033f-4b3f-b5ac-b44153da2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean2.printSchema()  # should show event_date: date\n",
    "# # root\n",
    "# #  |-- device_id: string (nullable = ...)\n",
    "# #  |-- ...\n",
    "# #  |-- event_ts: timestamp (nullable = ...)\n",
    "# #  |-- event_date: date (nullable = ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8771bc1e-05c7-4100-a7b2-f0c0a409af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unnamed> isActive: True lastProgress: {'id': '7c2e72b0-f218-4f1d-a6e0-50bae75b2d65', 'runId': 'f5f5d63b-aaae-41a9-9f63-75636c53d008', 'name': None, 'timestamp': '2025-08-24T13:20:01.216Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 7, 'triggerExecution': 7}, 'eventTime': {'watermark': '2025-08-24T08:32:25.018Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/invalid_status]', 'numOutputRows': -1}}\n",
      "<unnamed> isActive: True lastProgress: {'id': '34d92474-400c-48b5-939a-bed4a42f3b87', 'runId': '2f01f5eb-b870-403d-8ffd-42f7389ae9c6', 'name': None, 'timestamp': '2025-08-24T13:20:01.199Z', 'batchId': 9, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 11, 'triggerExecution': 11}, 'eventTime': {'watermark': '2025-08-24T08:32:25.299Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/curated/events]', 'numOutputRows': -1}}\n",
      "<unnamed> isActive: True lastProgress: {'id': '8df5cac2-c0f6-4c63-8773-a8e5eba8fc87', 'runId': 'e3452921-f9b6-41c6-af54-ba3b7cf75ee8', 'name': None, 'timestamp': '2025-08-24T13:20:01.199Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 10, 'triggerExecution': 10}, 'eventTime': {'watermark': '2025-08-23T23:55:00.000Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/invalid_temp]', 'numOutputRows': -1}}\n",
      "<unnamed> isActive: True lastProgress: {'id': 'ae462a92-c825-4e38-bb9f-e2d738cef1eb', 'runId': '52fd6529-fc45-4a2b-a860-085fd2ee5dea', 'name': None, 'timestamp': '2025-08-24T13:20:00.948Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 12, 'triggerExecution': 12}, 'eventTime': {'watermark': '1970-01-01T00:00:00.000Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/invalid_vibe]', 'numOutputRows': -1}}\n",
      "<unnamed> isActive: True lastProgress: {'id': '6e1a7a85-5df5-4bd1-a569-34ab181e1b6b', 'runId': '3b81cd78-8aed-420b-a4e7-f748774fd6a8', 'name': None, 'timestamp': '2025-08-24T13:20:01.168Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 5, 'triggerExecution': 6}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/corrupt_json]', 'numOutputRows': -1}}\n",
      "<unnamed> isActive: True lastProgress: {'id': '3e5428c7-8763-4a3e-bbfc-1620fd214b5d', 'runId': 'a1456045-5e87-4e62-b114-7f562c34b6f1', 'name': None, 'timestamp': '2025-08-24T13:20:04.132Z', 'batchId': 6, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 7, 'triggerExecution': 7}, 'eventTime': {'watermark': '2025-08-24T08:32:25.299Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "<unnamed> isActive: True lastProgress: {'id': '2888fd29-607f-464e-994b-9c14050dfef3', 'runId': 'f7d238e8-db90-416d-ad51-5fe29f9f39a8', 'name': None, 'timestamp': '2025-08-24T13:20:01.053Z', 'batchId': 7, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 12, 'triggerExecution': 12}, 'eventTime': {'watermark': '2025-08-24T08:32:25.299Z'}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[sensor_events]]', 'startOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'endOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'latestOffset': {'sensor_events': {'2': 1742, '1': 1970, '0': 2295}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'FileSink[s3a://rt-stream/dq/quarantine/missing_key]', 'numOutputRows': -1}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------+--------+-----------+-----------------+----+------------------------+----------+\n",
      "|bad_json|bad_status|bad_vibration|epoch_id|missing_key|out_of_range_temp|rows|ts                      |date      |\n",
      "+--------+----------+-------------+--------+-----------+-----------------+----+------------------------+----------+\n",
      "|0       |281       |0            |5       |0          |0                |2997|2025-08-24T08:44:58.455Z|2025-08-24|\n",
      "|0       |1         |0            |4       |0          |0                |3   |2025-08-24T08:43:01.830Z|2025-08-24|\n",
      "|0       |1         |1            |3       |1          |1                |3   |2025-08-24T08:33:38.253Z|2025-08-24|\n",
      "|0       |0         |0            |2       |0          |0                |1   |2025-08-24T08:27:31.690Z|2025-08-24|\n",
      "|0       |1         |1            |1       |1          |1                |3   |2025-08-24T08:22:32.232Z|2025-08-24|\n",
      "+--------+----------+-------------+--------+-----------+-----------------+----+------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "s3a://rt-stream/dq/quarantine/invalid_temp 2\n",
      "s3a://rt-stream/dq/quarantine/missing_key 2\n",
      "s3a://rt-stream/dq/quarantine/invalid_vibe 2\n",
      "s3a://rt-stream/dq/quarantine/invalid_status 284\n"
     ]
    }
   ],
   "source": [
    "# live status\n",
    "for q in spark.streams.active:\n",
    "    print(q.name or \"<unnamed>\", \"isActive:\", q.isActive, \"lastProgress:\", q.lastProgress)\n",
    "\n",
    "# curated rows exist?\n",
    "spark.read.parquet(\"s3a://rt-stream/curated/events\").count()\n",
    "\n",
    "# latest DQ summary rows\n",
    "from pyspark.sql import functions as F\n",
    "spark.read.json(\"s3a://rt-stream/dq/streaming_summary\").orderBy(F.desc(\"ts\")).show(5, False)\n",
    "\n",
    "# quarantine counts\n",
    "for p in [\n",
    "  \"s3a://rt-stream/dq/quarantine/invalid_temp\",\n",
    "  \"s3a://rt-stream/dq/quarantine/missing_key\",\n",
    "  \"s3a://rt-stream/dq/quarantine/invalid_vibe\",\n",
    "  \"s3a://rt-stream/dq/quarantine/invalid_status\",\n",
    "]:\n",
    "    try:\n",
    "        print(p, spark.read.parquet(p).count())\n",
    "    except:\n",
    "        print(p, 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
